## Valid bootstrap options (required): ubuntu, coreos, centos, none
## If the OS is not listed here, it means it doesn't require extra/bootstrap steps.
## In example, python is not available on 'coreos' so it must be installed before
## anything else. In the opposite, Debian has already all its dependencies fullfiled, then bootstrap_os should be set to `none`.
bootstrap_os: none

## Directory where etcd data stored
etcd_data_dir: /var/lib/etcd

## Directory where the binaries will be installed
bin_dir: /usr/local/bin

## The access_ip variable is used to define how other nodes should access
## the node.  This is used in flannel to allow other flannel nodes to see
## this node for example.  The access_ip is really useful AWS and Google
## environments where the nodes are accessed remotely by the "public" ip,
## but don't know about that address themselves.
#access_ip: 1.1.1.1


## External LB example config
## apiserver_loadbalancer_domain_name: "elb.some.domain"
#loadbalancer_apiserver:
#  address: 1.2.3.4
#  port: 1234

## Internal loadbalancers for apiservers
#loadbalancer_apiserver_localhost: true

## Local loadbalancer should use this port instead, if defined.
## Defaults to kube_apiserver_port (6443)
#nginx_kube_apiserver_port: 8443

### OTHER OPTIONAL VARIABLES
## For some things, kubelet needs to load kernel modules.  For example, dynamic kernel services are needed
## for mounting persistent volumes into containers.  These may not be loaded by preinstall kubernetes
## processes.  For example, ceph and rbd backed volumes.  Set to true to allow kubelet to load kernel
## modules.
#kubelet_load_modules: false

## Upstream dns servers used by dnsmasq
#upstream_dns_servers:
#  - 8.8.8.8
#  - 8.8.4.4

## There are some changes specific to the cloud providers
## for instance we need to encapsulate packets with some network plugins
## If set the possible values are either 'gce', 'aws', 'azure', 'openstack', 'vsphere', 'oci', or 'external'
## When openstack is used make sure to source in the openstack credentials
## like you would do when using nova-client before starting the playbook.
#cloud_provider:


## Uncomment to enable experimental kubeadm deployment mode
#kubeadm_enabled: false

## Set these proxy values in order to update package manager and docker daemon to use proxies
#http_proxy: ""
#https_proxy: ""

## Refer to roles/kubespray-defaults/defaults/main.yml before modifying no_proxy
#no_proxy: ""

## Some problems may occur when downloading files over https proxy due to ansible bug
## https://github.com/ansible/ansible/issues/32750. Set this variable to False to disable
## SSL validation of get_url module. Note that kubespray will still be performing checksum validation.
#download_validate_certs: False

## If you need exclude all cluster nodes from proxy and other resources, add other resources here.
#additional_no_proxy: ""

## Certificate Management
## This setting determines whether certs are generated via scripts or whether a
## cluster of Hashicorp's Vault is started to issue certificates (using etcd
## as a backend). Options are "script" or "vault"
#cert_management: script

## Set to true to allow pre-checks to fail and continue deployment
#ignore_assert_errors: false

## The read-only port for the Kubelet to serve on with no authentication/authorization. Uncomment to enable.
#kube_read_only_port: 10255

## Set true to download and cache container
#download_container: true


## SEBA Offline Repo Overrides

docker_ubuntu_repo_base_url: "http://repo.seba.local/repo"
docker_ubuntu_repo_gpgkey: "http://repo.seba.local/repo/key"

kubeadm_download_url: "http://repo.seba.local/k8s/kubeadm"
vault_download_url: "http://repo.seba.local/k8s/vault_0.10.1_linux_amd64.zip"
etcd_download_url: "http://repo.seba.local/k8s/etcd-v3.2.18-linux-amd64.tar.gz"
hyperkube_download_url: "http://repo.seba.local/k8s/hyperkube"

etcd_image_repo: "repo.seba.local:5000/quay.io/coreos/etcd"
flannel_image_repo: "repo.seba.local:5000/quay.io/coreos/flannel"
flannel_cni_image_repo: "repo.seba.local:5000/quay.io/coreos/flannel-cni"
calicoctl_image_repo: "repo.seba.local:5000/quay.io/calico/ctl"
calico_node_image_repo: "repo.seba.local:5000/quay.io/calico/node"
calico_cni_image_repo: "repo.seba.local:5000/quay.io/calico/cni"
calico_policy_image_repo: "repo.seba.local:5000/quay.io/calico/kube-controllers"
calico_rr_image_repo: "repo.seba.local:5000/quay.io/calico/routereflector"
hyperkube_image_repo: "repo.seba.local:5000/gcr.io/google-containers/hyperkube-amd64"
pod_infra_image_repo: "repo.seba.local:5000/gcr.io/google_containers/pause-amd64"
install_socat_image_repo: "repo.seba.local:5000/xueshanf/install-socat"
netcheck_agent_image_repo: "repo.seba.local:5000/mirantis/k8s-netchecker-agent"
netcheck_server_image_repo: "repo.seba.local:5000/mirantis/k8s-netchecker-server"
weave_kube_image_repo: "repo.seba.local:5000/docker.io/weaveworks/weave-kube"
weave_npc_image_repo: "repo.seba.local:5000/docker.io/weaveworks/weave-npc"
contiv_image_repo: "repo.seba.local:5000/contiv/netplugin"
contiv_init_image_repo: "repo.seba.local:5000/contiv/netplugin-init"
contiv_auth_proxy_image_repo: "repo.seba.local:5000/contiv/auth_proxy"
contiv_etcd_init_image_repo: "repo.seba.local:5000/ferest/etcd-initer"
contiv_ovs_image_repo: "repo.seba.local:5000/contiv/ovs"
cilium_image_repo: "repo.seba.local:5000/docker.io/cilium/cilium"
nginx_image_repo: "repo.seba.local:5000/nginx"
dnsmasq_image_repo: "repo.seba.local:5000/andyshinn/dnsmasq"
kubedns_image_repo: "repo.seba.local:5000/gcr.io/google_containers/k8s-dns-kube-dns-amd64"
coredns_image_repo: "repo.seba.local:5000/gcr.io/google-containers/coredns"
dnsmasq_nanny_image_repo: "repo.seba.local:5000/gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64"
dnsmasq_sidecar_image_repo: "repo.seba.local:5000/gcr.io/google_containers/k8s-dns-sidecar-amd64"
dnsmasqautoscaler_image_repo: "repo.seba.local:5000/gcr.io/google_containers/cluster-proportional-autoscaler-amd64"
kubednsautoscaler_image_repo: "repo.seba.local:5000/gcr.io/google_containers/cluster-proportional-autoscaler-amd64"
test_image_repo: "repo.sebal.local:5000/busybox"
helm_image_repo: "repo.seba.local:5000/lachlanevenson/k8s-helm"
tiller_image_repo: "repo.seba.local:5000/gcr.io/kubernetes-helm/tiller"
vault_image_repo: "repo.seba.local:5000/vault"
registry_image_repo: "repo.seba.local:5000/registry"
registry_proxy_image_repo: "repo.seba.local:5000/gcr.io/google_containers/kube-registry-proxy"
local_volume_provisioner_image_repo: "repo.seba.local:5000/quay.io/external_storage/local-volume-provisioner"
cephfs_provisioner_image_repo: "repo.seba.local:5000/quay.io/external_storage/cephfs-provisioner"
ingress_nginx_controller_image_repo: "repo.seba.local:5000/quay.io/kubernetes-ingress-controller/nginx-ingress-controller"
ingress_nginx_default_backend_image_repo: "repo.seba.local:5000/gcr.io/google_containers/defaultbackend"
cert_manager_controller_image_repo: "repo.seba.local:5000/quay.io/jetstack/cert-manager-controller"
dashboard_image_repo: "repo.seba.local:5000/gcr.io/google_containers/kubernetes-dashboard-amd64"

helm_stable_repo_url: "http://repo.seba.local/charts/"
